# Testing

After implementing a new feature or fixing a bug, one can run all the tests to see if anything is broken.

To run all the tests with each Python version (3.10, 3.11, 3.12, and 3.13), use the following command.

```bash
hatch run test:test
```

To run the tests with a specific Python version, use the following command.

```bash
hatch run test.py3.10:test
```

To run the tests with Python 3.13 and generate a coverage report, use the following command.

```bash
hatch run test-and-report
```
Once new commits are pushed to the `main` branch, the [`test.yaml`](https://github.com/rendercv/rendercv/blob/main/.github/workflows/test.yaml) workflow will be automatically triggered, and the tests will run.

## About [`testdata`](https://github.com/rendercv/rendercv/tree/main/tests/testdata) folder

In some of the tests:

- RenderCV generates an output with a sample input.
- Then, the output is compared with a reference output, which has been manually generated and stored in `testdata`. If the files differ, the tests fail.


When the `testdata` folder needs to be updated, it can be manually regenerated by setting `update_testdata` to `True` in `conftest.py` and running the tests.

!!! warning
    - Whenever the `testdata` folder is generated, the files should be reviewed manually to ensure everything works as expected.
    - `update_testdata` should be set to `False` before committing the changes.


# Writing Tests

This guide explains how to write tests for RenderCV. Follow these patterns to keep the test suite consistent and maintainable.

## Test Organization

Tests live in `tests/` and fall into two categories:

1. **Unit tests**: Mirror the `src/rendercv/` structure (e.g., `tests/renderer/`, `tests/schema/`, `tests/cli/`)
2. **Non-unit tests**: `tests/integration/`, `tests/scripts/`

## Unit Tests

### File Structure

One test file per source file, mirroring the folder structure:

```
src/rendercv/renderer/templater/date.py
    → tests/renderer/templater/test_date.py

src/rendercv/schema/models/cv/section.py
    → tests/schema/models/cv/test_section.py
```

### Naming Conventions

Test names must include the name of the function or class being tested.

**When you need only one test**, use `test_` + the name:
- Testing `clean_url()` → `test_clean_url`
- Testing `Cv` → `test_cv`

**When you need multiple tests**, wrap them in a class using `Test` + PascalCase name:
- Testing `clean_url()` → `TestCleanUrl`
- Testing `Cv` → `TestCv`

Example with one test:

```python
@pytest.mark.parametrize(
    ("url", "expected_clean_url"),
    [
        ("https://example.com", "example.com"),
        ("https://example.com/", "example.com"),
        ("https://example.com/test", "example.com/test"),
    ],
)
def test_clean_url(url, expected_clean_url):
    assert clean_url(url) == expected_clean_url
```

Example with multiple tests:

```python
class TestComputeDateString:
    @pytest.mark.parametrize(...)
    def test_date_parameter_takes_precedence(self, ...):
        ...

    @pytest.mark.parametrize(...)
    def test_date_ranges(self, ...):
        ...

    @pytest.mark.parametrize(...)
    def test_returns_none_for_incomplete_data(self, ...):
        ...

### Use Parametrize for Variations

Instead of writing multiple similar tests, use `@pytest.mark.parametrize`:

```python
@pytest.mark.parametrize(
    ("input_a", "input_b", "expected"),
    [
        ("2020-01-01", "2021-01-01", "Jan 2020 – Jan 2021"),
        ("2020-01", "2021-02-01", "Jan 2020 – Feb 2021"),
        (2020, 2021, "2020 – 2021"),
    ],
)
def test_date_ranges(self, input_a, input_b, expected):
    result = compute_date_string(None, input_a, input_b, EnglishLocale())
    assert result == expected
```

### Shared Fixtures with conftest.py

Place shared fixtures in `conftest.py`. Use the closest one possible:

- Fixtures for one folder → that folder's `conftest.py`
- Fixtures for multiple folders → their closest common parent's `conftest.py`

```
tests/
├── conftest.py                    # Used across all tests
├── schema/
│   ├── conftest.py                # Used by schema tests only
│   └── models/
│       └── cv/
│           ├── conftest.py        # Used by CV model tests only
│           ├── test_section.py
│           └── test_cv.py
└── renderer/
    └── ...
```

### Guidelines

**Keep tests focused.** Test functions in isolation: input → output.

**Don't create unnecessary fixtures.** If setup is one clear line, inline it:

```python
# Don't:
@pytest.fixture
def locale(self):
    return EnglishLocale()

def test_something(self, locale):
    result = format_date(Date(2020, 1, 1), locale)

# Do:
def test_something(self):
    result = format_date(Date(2020, 1, 1), EnglishLocale())
```

**Prefer real behavior over mocking.** Only mock when there's no practical alternative.

**Name tests by what the function should do, not what you're passing in:**
- Good: `test_returns_none_for_incomplete_data` — tells you the expected behavior
- Unclear: `test_function_with_none_input` — tells you nothing about what should happen

**Keep tests simple:**

```python
def test_something(self, input, expected):
    result = function_under_test(input)
    assert result == expected
```

**Add docstrings when something isn't self-explanatory.** This applies to both tests and fixtures. If a new developer would look at it and wonder "what is this doing?" or "why is it done this way?"—add a short docstring. Keep it to the minimum necessary: what it does, why it exists, why it's implemented that way if non-obvious.

### What to Test

- Inputs → expected outputs
- Inputs → expected errors

### Patterns to Avoid

- Testing implementation details rather than behavior
- Writing integration tests disguised as unit tests
- Creating complex fixtures when simple values work

## Non-Unit Tests

<!-- TODO -->